{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.read_csv(\"../Data/Clean_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Is_SentimentHeadline_Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>obama lay wreath arlington national cemetery p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tim haywood investment director businessunit h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nouriel roubini nyu professor chairman roubini...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>finland economy expand marginally month end de...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tourism public spending continue boost economy...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  \\\n",
       "0  obama lay wreath arlington national cemetery p...   \n",
       "1  tim haywood investment director businessunit h...   \n",
       "2  nouriel roubini nyu professor chairman roubini...   \n",
       "3  finland economy expand marginally month end de...   \n",
       "4  tourism public spending continue boost economy...   \n",
       "\n",
       "   Is_SentimentHeadline_Positive  \n",
       "0                              0  \n",
       "1                              0  \n",
       "2                              1  \n",
       "3                              1  \n",
       "4                              1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = news_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_nv, X_test_nv, y_train, y_test = train_test_split(news_df['Headline'], news_df['Is_SentimentHeadline_Positive'], \n",
    "                                                    train_size=0.8, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=4000, min_df=7, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X_train = vectorizer.fit_transform(X_train_nv).toarray()\n",
    "X_test = vectorizer.transform(X_test_nv)\n",
    "\n",
    "# Feature Selection\n",
    "chi2_selector = SelectKBest(chi2, k=2000)\n",
    "X_train = chi2_selector.fit_transform(X_train, y_train)\n",
    "X_test = chi2_selector.transform(X_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the vectorizer and feature Selector\n",
    "import pickle\n",
    "\n",
    "pickle.dump(vectorizer, open(\"../App/Model/vectorizer.pkl\", \"wb\"))\n",
    "pickle.dump(chi2_selector, open(\"../App/Model/chi_selector.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Regression Algorithm\n",
      "\n",
      "Confusion Matrix: \n",
      " [[5909 1161]\n",
      " [2097 2785]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.84      0.78      7070\n",
      "           1       0.71      0.57      0.63      4882\n",
      "\n",
      "    accuracy                           0.73     11952\n",
      "   macro avg       0.72      0.70      0.71     11952\n",
      "weighted avg       0.72      0.73      0.72     11952\n",
      "\n",
      "Accuracy Score: \n",
      " 0.7274096385542169\n",
      "\n",
      "Stochastic Gradient Descent Algorithm\n",
      "\n",
      "Confusion Matrix: \n",
      " [[6310  760]\n",
      " [2694 2188]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.89      0.79      7070\n",
      "           1       0.74      0.45      0.56      4882\n",
      "\n",
      "    accuracy                           0.71     11952\n",
      "   macro avg       0.72      0.67      0.67     11952\n",
      "weighted avg       0.72      0.71      0.69     11952\n",
      "\n",
      "Accuracy Score: \n",
      " 0.7110107095046854\n",
      "\n",
      "Random Forest Classifier  Algorithm\n",
      "\n",
      "Confusion Matrix: \n",
      " [[5683 1387]\n",
      " [2106 2776]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.80      0.76      7070\n",
      "           1       0.67      0.57      0.61      4882\n",
      "\n",
      "    accuracy                           0.71     11952\n",
      "   macro avg       0.70      0.69      0.69     11952\n",
      "weighted avg       0.70      0.71      0.70     11952\n",
      "\n",
      "Accuracy Score: \n",
      " 0.7077476572958501\n",
      "\n",
      "Multinomial Naive Bayes Algorithm\n",
      "\n",
      "Confusion Matrix: \n",
      " [[6323  747]\n",
      " [3035 1847]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.89      0.77      7070\n",
      "           1       0.71      0.38      0.49      4882\n",
      "\n",
      "    accuracy                           0.68     11952\n",
      "   macro avg       0.69      0.64      0.63     11952\n",
      "weighted avg       0.69      0.68      0.66     11952\n",
      "\n",
      "Accuracy Score: \n",
      " 0.6835676037483266\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import *\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "LR = LogisticRegression()\n",
    "SGDC = SGDClassifier()\n",
    "RFC = RandomForestClassifier(n_estimators=300, random_state=0)\n",
    "MNB = MultinomialNB()\n",
    "\n",
    "# Logistic Regression\n",
    "LR.fit(X_train, y_train)\n",
    "LR_Model = LR.predict(X_test)\n",
    "print(\"\\nLinear Regression Algorithm\\n\")\n",
    "print(\"Confusion Matrix: \\n\",confusion_matrix(y_test,LR_Model))\n",
    "print(\"Classification Report: \\n\",classification_report(y_test,LR_Model))\n",
    "print(\"Accuracy Score: \\n\",accuracy_score(y_test, LR_Model))\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "SGDC.fit(X_train, y_train)\n",
    "SGDC_Model = SGDC.predict(X_test)\n",
    "print(\"\\nStochastic Gradient Descent Algorithm\\n\")\n",
    "print(\"Confusion Matrix: \\n\",confusion_matrix(y_test,SGDC_Model))\n",
    "print(\"Classification Report: \\n\",classification_report(y_test,SGDC_Model))\n",
    "print(\"Accuracy Score: \\n\",accuracy_score(y_test, SGDC_Model))\n",
    "\n",
    "# Random Forest Classifier \n",
    "RFC.fit(X_train, y_train)\n",
    "RFC_Model = RFC.predict(X_test)\n",
    "print(\"\\nRandom Forest Classifier  Algorithm\\n\")\n",
    "print(\"Confusion Matrix: \\n\",confusion_matrix(y_test,RFC_Model))\n",
    "print(\"Classification Report: \\n\",classification_report(y_test,RFC_Model))\n",
    "print(\"Accuracy Score: \\n\",accuracy_score(y_test, RFC_Model))\n",
    "\n",
    "# Multinomial Naive Bayes \n",
    "MNB.fit(X_train, y_train)\n",
    "MNB_Model = MNB.predict(X_test)\n",
    "print(\"\\nMultinomial Naive Bayes Algorithm\\n\")\n",
    "print(\"Confusion Matrix: \\n\",confusion_matrix(y_test,MNB_Model))\n",
    "print(\"Classification Report: \\n\",classification_report(y_test,MNB_Model))\n",
    "print(\"Accuracy Score: \\n\",accuracy_score(y_test, MNB_Model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "pickle.dump(LR, open(\"../App/Model/logistic_regression.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model using Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data and vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import regularizers\n",
    "\n",
    "max_words = 5000\n",
    "max_len = 200\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train_nv)\n",
    "sequences = tokenizer.texts_to_sequences(X_train_nv)\n",
    "X_train = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test_nv)\n",
    "X_test = pad_sequences(test_sequences, maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import *\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rnn_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(5000, 100, input_length = 200))\n",
    "    model.add(LSTM(256))\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 200, 100)          500000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 256)               365568    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,000,737\n",
      "Trainable params: 999,201\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = get_rnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2391/2391 [==============================] - 817s 341ms/step - loss: 0.8134 - accuracy: 0.5376 - val_loss: 0.6799 - val_accuracy: 0.5915\n",
      "Epoch 2/50\n",
      "2391/2391 [==============================] - 872s 365ms/step - loss: 0.6965 - accuracy: 0.5525 - val_loss: 0.6768 - val_accuracy: 0.5915\n",
      "Epoch 3/50\n",
      "2391/2391 [==============================] - 852s 356ms/step - loss: 0.6882 - accuracy: 0.5683 - val_loss: 0.5845 - val_accuracy: 0.6885\n",
      "Epoch 4/50\n",
      "2391/2391 [==============================] - 879s 367ms/step - loss: 0.5756 - accuracy: 0.7001 - val_loss: 0.5612 - val_accuracy: 0.7195\n",
      "Epoch 5/50\n",
      "2391/2391 [==============================] - 841s 352ms/step - loss: 0.5237 - accuracy: 0.7422 - val_loss: 0.5401 - val_accuracy: 0.7274\n",
      "Epoch 6/50\n",
      "2391/2391 [==============================] - 877s 367ms/step - loss: 0.4913 - accuracy: 0.7667 - val_loss: 0.5553 - val_accuracy: 0.7208\n",
      "Epoch 7/50\n",
      "2391/2391 [==============================] - 847s 354ms/step - loss: 0.4625 - accuracy: 0.7833 - val_loss: 0.5847 - val_accuracy: 0.7159\n",
      "Epoch 8/50\n",
      "2391/2391 [==============================] - 934s 391ms/step - loss: 0.4268 - accuracy: 0.8027 - val_loss: 0.5735 - val_accuracy: 0.7163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a830417430>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate)\n",
    "\n",
    "model1.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "verbose = 1\n",
    "epochs = 50\n",
    "batch_size = 20\n",
    "validation_split = 0.2\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "model1.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=verbose,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks = [callback]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374/374 [==============================] - 55s 148ms/step\n",
      "Accuracy: 0.716282\n",
      "Precision: 0.674223\n",
      "Recall: 0.590946\n",
      "F1 score: 0.629844\n"
     ]
    }
   ],
   "source": [
    "# predict probabilities for test set\n",
    "y_classes = model1.predict_classes(X_test, verbose=1)\n",
    "# reduce to 1d array\n",
    "y_classes = y_classes[:, 0]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "\n",
    "precision = precision_score(y_test, y_classes)\n",
    "print('Precision: %f' % precision)\n",
    "\n",
    "recall = recall_score(y_test, y_classes)\n",
    "print('Recall: %f' % recall)\n",
    "\n",
    "f1 = f1_score(y_test, y_classes)\n",
    "print('F1 score: %f' % f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_model():   \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(max_words, 100, input_length=200))\n",
    "    \n",
    "    model.add(Conv1D(1024, 3, padding='valid', activation='relu', strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    \n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(2048, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 100)          500000    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 198, 1024)         308224    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2048)              2099200   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 2,921,761\n",
      "Trainable params: 2,915,617\n",
      "Non-trainable params: 6,144\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = get_cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2391/2391 [==============================] - 388s 162ms/step - loss: 0.8611 - accuracy: 0.5736 - val_loss: 0.5670 - val_accuracy: 0.7095\n",
      "Epoch 2/50\n",
      "2391/2391 [==============================] - 351s 147ms/step - loss: 0.6046 - accuracy: 0.6923 - val_loss: 0.5710 - val_accuracy: 0.7194\n",
      "Epoch 3/50\n",
      "2391/2391 [==============================] - 297s 124ms/step - loss: 0.5688 - accuracy: 0.7212 - val_loss: 0.5557 - val_accuracy: 0.7144\n",
      "Epoch 4/50\n",
      "2391/2391 [==============================] - 309s 129ms/step - loss: 0.5479 - accuracy: 0.7338 - val_loss: 0.5604 - val_accuracy: 0.7175\n",
      "Epoch 5/50\n",
      "2391/2391 [==============================] - 326s 136ms/step - loss: 0.5286 - accuracy: 0.7447 - val_loss: 0.5670 - val_accuracy: 0.7177\n",
      "Epoch 6/50\n",
      "2391/2391 [==============================] - 347s 145ms/step - loss: 0.5118 - accuracy: 0.7534 - val_loss: 0.5614 - val_accuracy: 0.7207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a837372d30>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate)\n",
    "\n",
    "model2.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "verbose = 1\n",
    "epochs = 50\n",
    "batch_size = 20\n",
    "validation_split = 0.2\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "model2.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=verbose,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks = [callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374/374 [==============================] - 19s 51ms/step\n",
      "Accuracy: 0.720716\n",
      "Precision: 0.651551\n",
      "Recall: 0.679844\n",
      "F1 score: 0.665397\n"
     ]
    }
   ],
   "source": [
    "# predict probabilities for test set\n",
    "y_classes = model2.predict_classes(X_test, verbose=1)\n",
    "# reduce to 1d array\n",
    "y_classes = y_classes[:, 0]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "\n",
    "precision = precision_score(y_test, y_classes)\n",
    "print('Precision: %f' % precision)\n",
    "\n",
    "recall = recall_score(y_test, y_classes)\n",
    "print('Recall: %f' % recall)\n",
    "\n",
    "f1 = f1_score(y_test, y_classes)\n",
    "print('F1 score: %f' % f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
