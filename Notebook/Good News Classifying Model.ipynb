{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the dataframe\n",
    "news_df = pd.read_csv(\"../Data/Clean_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Is_SentimentTitle_Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>look health chinese economy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nouriel roubini global economy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fire claim   barn hancock county</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>big datum internet thing add   uk economy   re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>china share economy generate usd</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0                        look health chinese economy   \n",
       "1                   nouriel roubini global economy     \n",
       "2                   fire claim   barn hancock county   \n",
       "3  big datum internet thing add   uk economy   re...   \n",
       "4                 china share economy generate usd     \n",
       "\n",
       "   Is_SentimentTitle_Positive  \n",
       "0                           1  \n",
       "1                           0  \n",
       "2                           0  \n",
       "3                           1  \n",
       "4                           0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the word to it's similar more common word\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import en_core_web_lg \n",
    "import json\n",
    "\n",
    "def consolidate_words(textlist):\n",
    "    nlp = en_core_web_lg.load()\n",
    "    \n",
    "    list_of_text = textlist.copy()\n",
    "    \n",
    "    wordcounts = Counter(' '.join(list_of_text).split())    \n",
    "    low_words = [k for k, v in wordcounts.items() if v <= 1]\n",
    "    other_words = [k for k, v in wordcounts.items() if v > 1]\n",
    "    \n",
    "    tokens = nlp(' '.join(other_words))\n",
    "    \n",
    "    replacement_dict = {}\n",
    "    \n",
    "    for word in low_words:\n",
    "\n",
    "        word_token = nlp(word)\n",
    "        max_similarity = 0.8\n",
    "\n",
    "        for tk in tokens:\n",
    "            # find the maximum similarity above threshold\n",
    "            sim_score = word_token.similarity(tk)\n",
    "\n",
    "            if 1 > sim_score > max_similarity:\n",
    "                replacement_dict[word] = tk.text\n",
    "                max_similarity = sim_score\n",
    "                            \n",
    "        try:\n",
    "            replacement_dict[word]      \n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    return replacement_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_words = [word for line in list(news_df[\"Title\"]) for word in line.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#consolidate_words_dict = consolidate_words(list_of_words)\n",
    "#with open('replacement.json', 'w') as fp:\n",
    "#    json.dump(consolidate_words_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "with open('replacement.json') as json_file:\n",
    "    consolidate_words_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_consolidate(text):\n",
    "    text = text.strip()\n",
    "    str_=\"\"\n",
    "    for word in text.split(\" \"):\n",
    "        try:\n",
    "            word = consolidate_words_dict[word]\n",
    "        except:\n",
    "            pass\n",
    "        str_ += word +\" \"\n",
    "    return str_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#news_df['Title'] = news_df['Title'].apply(replace_consolidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(news_df['Title'], news_df['Is_SentimentTitle_Positive'], \n",
    "                                                    train_size=0.8, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=4000, min_df=7, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X_train = vectorizer.fit_transform(X_train).toarray()\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Feature Selection\n",
    "chi2_selector = SelectKBest(chi2, k=2000)\n",
    "X_train = chi2_selector.fit_transform(X_train, y_train)\n",
    "X_test = chi2_selector.transform(X_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Regression Algorithm\n",
      "\n",
      "Confusion Matrix: \n",
      " [[3513 1029]\n",
      " [1256 2880]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.77      0.75      4542\n",
      "           1       0.74      0.70      0.72      4136\n",
      "\n",
      "    accuracy                           0.74      8678\n",
      "   macro avg       0.74      0.73      0.74      8678\n",
      "weighted avg       0.74      0.74      0.74      8678\n",
      "\n",
      "Accuracy Score: \n",
      " 0.7366904816778059\n",
      "\n",
      "Stochastic Gradient Descent Algorithm\n",
      "\n",
      "Confusion Matrix: \n",
      " [[3461 1081]\n",
      " [1238 2898]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.76      0.75      4542\n",
      "           1       0.73      0.70      0.71      4136\n",
      "\n",
      "    accuracy                           0.73      8678\n",
      "   macro avg       0.73      0.73      0.73      8678\n",
      "weighted avg       0.73      0.73      0.73      8678\n",
      "\n",
      "Accuracy Score: \n",
      " 0.7327725282323115\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import *\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "SGDC = SGDClassifier()\n",
    "SVCM = SVC()\n",
    "LR = LogisticRegression()\n",
    "RFC = RandomForestClassifier(n_estimators=300, random_state=0)\n",
    "MNB = MultinomialNB()\n",
    "\n",
    "# Linear Regression\n",
    "LR.fit(X_train, y_train)\n",
    "LR_Model = LR.predict(X_test)\n",
    "print(\"\\nLinear Regression Algorithm\\n\")\n",
    "print(\"Confusion Matrix: \\n\",confusion_matrix(y_test,LR_Model))\n",
    "print(\"Classification Report: \\n\",classification_report(y_test,LR_Model))\n",
    "print(\"Accuracy Score: \\n\",accuracy_score(y_test, LR_Model))\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "SGDC.fit(X_train, y_train)\n",
    "SGDC_Model = SGDC.predict(X_test)\n",
    "print(\"\\nStochastic Gradient Descent Algorithm\\n\")\n",
    "print(\"Confusion Matrix: \\n\",confusion_matrix(y_test,SGDC_Model))\n",
    "print(\"Classification Report: \\n\",classification_report(y_test,SGDC_Model))\n",
    "print(\"Accuracy Score: \\n\",accuracy_score(y_test, SGDC_Model))\n",
    "\n",
    "# Support Vector Model\n",
    "SVCM.fit(X_train, y_train)\n",
    "SVCM_Model = SVCM.predict(X_test)\n",
    "print(\"\\nSupport Vector Method Algorithm\\n\")\n",
    "print(\"Confusion Matrix: \\n\",confusion_matrix(y_test,SVCM_Model))\n",
    "print(\"Classification Report: \\n\",classification_report(y_test,SVCM_Model))\n",
    "print(\"Accuracy Score: \\n\",accuracy_score(y_test, SVCM_Model))\n",
    "\n",
    "# Random Forest Classifier \n",
    "RFC.fit(X_train, y_train)\n",
    "RFC_Model = RFC.predict(X_test)\n",
    "print(\"\\nRandom Forest Classifier  Algorithm\\n\")\n",
    "print(\"Confusion Matrix: \\n\",confusion_matrix(y_test,RFC_Model))\n",
    "print(\"Classification Report: \\n\",classification_report(y_test,RFC_Model))\n",
    "print(\"Accuracy Score: \\n\",accuracy_score(y_test, RFC_Model))\n",
    "\n",
    "# Multinomial Naive Bayes \n",
    "MNB.fit(X_train, y_train)\n",
    "MNB_Model = MNB.predict(X_test)\n",
    "print(\"\\nMultinomial Naive Bayes Algorithm\\n\")\n",
    "print(\"Confusion Matrix: \\n\",confusion_matrix(y_test,MNB_Model))\n",
    "print(\"Classification Report: \\n\",classification_report(y_test,MNB_Model))\n",
    "print(\"Accuracy Score: \\n\",accuracy_score(y_test, MNB_Model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
